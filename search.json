[
  {
    "objectID": "wi/testing.html",
    "href": "wi/testing.html",
    "title": "Writing function tests",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\nAuthored/revised by:\n\n\n\n\n\n\n\n\nName\nDate\n\n\n\n\nAlan Haynes,1 Elio Carreras2\n2025-01-01\n\n\n\n\nVersion History\n\n\n\n\n\n\n\n\n\n\nVersion\nDate\nAuthor(s)\nSummary of Changes\n\n\n\n\n1.0\n2025-01-01\nAlan Haynes,\nInitial version\nThe validation package contains a set of functions to assist in the running and reporting of tests that have been published on the SCTO platform. The tests themselves are located in the validation_tests repository. The results of the tests are then published as issues in the pkg_validation repository.\nTests are written using functions from the testthat package, and can be downloaded, run, and reported using the functions in the validation package.",
    "crumbs": [
      "Work Intructions",
      "Writing function tests"
    ]
  },
  {
    "objectID": "wi/testing.html#high-level-summary",
    "href": "wi/testing.html#high-level-summary",
    "title": "Writing function tests",
    "section": "High level summary",
    "text": "High level summary\nThis section gives a very high level overview of the steps in the process. Refer to the following sections for more in depth information.\n\nfork the validation_tests repository to your own GitHub account\nclone the forked repository to your local machine\nuse validation::test_skeleton() to create the necessary files for testing a new package or function\nwrite the tests using the testthat package\ntest your new tests with validation::test()\ncommit your changes to git and push them to your GitHub fork\ncreate a pull request to the validation_tests repository\naddress any issues highlighted by the reviewer",
    "crumbs": [
      "Work Intructions",
      "Writing function tests"
    ]
  },
  {
    "objectID": "wi/testing.html#core-principles",
    "href": "wi/testing.html#core-principles",
    "title": "Writing function tests",
    "section": "Core principles",
    "text": "Core principles\n\nthe unit of testing is the function, not the package\nit is not necessary to test all functions, only those that are used in the product\n\ne.g. assuming that the lme4 package is classified as being high risk, it may be that initially only lmer is tested.\nSome time later, glmer is used in a product, and so it is necessary to test this function as well.\n\ntests should be written in a way that they can be run automatically\ntests are written using the testthat package\ntests are documented sufficiently to allow others to understand what is being tested and why (e.g. the documentation establishes the user requirements)\n\nthis principle is not thoroughly applied in this document as it is informative only. See existing examples in the repository.",
    "crumbs": [
      "Work Intructions",
      "Writing function tests"
    ]
  },
  {
    "objectID": "wi/testing.html#test_skeleton-helps-build-the-structure",
    "href": "wi/testing.html#test_skeleton-helps-build-the-structure",
    "title": "Writing function tests",
    "section": "test_skeleton helps build the structure",
    "text": "test_skeleton helps build the structure\nThe test_skeleton function can be used to create the relevant folder structure for testing a new package, or adding a file for testing additional functions. In the code below, substitute pkg with the name of the package to be tested and add the names of the function(s) you want to test in the funs argument.\n\ntest_skeleton(\"pkg\", funs = c(\"fun\", \"fun2\", \"etc\"))\n\nThis will create a set of files in your working directory:\n-- pkg\n   +- info.txt\n   +- setup-pkg.R\n   +- test-fun.R\n   +- test-fun2.R\n   +- test-etc.R\n\ninfo.txt file will contain the name of the package and a freetext description of what is tested,\nsetup-pkg.R is for any necessary setup code (e.g. installation of the package),\nfor each function in the funs argument, a test-function.R file is created, which will contain the actual testing code.\n\nIn the event that the package already has tests, the test_skeleton function will not overwrite the existing files, only adding any necessary test-function.R files.\nAdd the relevant tests to the test-function.R files and check that they work as expected (run test(\"package\", download = FALSE)).",
    "crumbs": [
      "Work Intructions",
      "Writing function tests"
    ]
  },
  {
    "objectID": "wi/testing.html#writing-tests",
    "href": "wi/testing.html#writing-tests",
    "title": "Writing function tests",
    "section": "Writing tests",
    "text": "Writing tests\nTesting is performed via the testthat framework. All tests for a given function should be placed in a dedicated test-function.R file.\nEach test is comprised of one or more expectations and a descriptive name.\nE.g.\ntest_that(\"some meaningful message about the tests\", {\n  expect_equal(1 + 1, 2)\n  expect_true(is.numeric(1))\n  expect_false(is.character(1))\n})\nWhere multiple tests are to be made on what could be a single object, it is often useful to create the object outside of the test_that function. This is particularly useful when different descriptive texts should be shown for the tests (e.g. perhaps the coefficients and standard errors from a model):\nobj &lt;- some_function(params)\ntest_that(\"test 1 on obj\", {\n  expect_equal(obj$value_to_test, expected_value)\n})\ntest_that(\"test 2 on obj\", {\n  expect_equal(obj$another_value_to_test, expected_value)\n})\nIf objects are only useful to the test, they can be created within the test_that function.\ntest_that(\"tests on obj\", {\n  obj &lt;- some_function(params)\n  expect_equal(obj$value_to_test, expected_value)\n  expect_equal(obj$another_value_to_test, expected_value)\n})\nMaking the description of the test meaningful is important, as it will help the user diagnose where the problem is.\ntestthat supports a large number of expectations, which are documented in the testthat documentation. We demonstrate a few examples below.\n\nCompare computation to a reference value\nTo test the computation of the function, the following code must be added to the testing file, for as many test cases as considered appropriate:\ntest_that(\"function f works\", {\n  expect_equal(f(x), y)\n})\nWhere f is the function to be tested, x are the input parameters for the function and y is the expected returned value.\nNote that is/may be necessary/desirable to set a tolerance for floating point comparisons. This can be done with the tolerance argument.\n\n\nTesting for errors, warnings and other messages\nTo test whether, under certain conditions, the function returns an error, a warning or a message, the following corresponding code can be adapted, for as many test cases as considered appropriate:\ntest_that(\"function f returns an error\", {\n  expect_error(f(x))\n})\n\ntest_that(\"function f returns a warning\", {\n  expect_warning(f(x))\n})\n\ntest_that(\"function f returns a message\", {\n  expect_message(f(x))\n})\nWhere f is the function to be tested, x are the arguments that define the conditions. Use the regexp argument to check for a particular error, warning or message.\ntest_that(\"function f returns an error\", {\n  expect_error(f(x), regexp = \"some error message\")\n})\nIn contrast, to test whether the function runs without returning an error, a warning or a message, the following corresponding code can be adapted, for as many test cases as considered appropriate:\ntest_that(\"function f runs without error\", {\n  expect_no_error(f(x))\n})\n\ntest_that(\"function f runs without a warning\", {\n  expect_warning(f(x))\n})\n\ntest_that(\"function f runs without a message\", {\n  expect_message(f(x))\n})\n\n\nTesting booleans\nTo test whether, under certain conditions, the function returns TRUE or FALSE, adapt the following code as appropriate:\ntest_that(\"function f returns TRUE\", {\n  expect_true(f(x))\n})\n\ntest_that(\"function f returns FALSE\", {\n  expect_false(f(x))\n})\nWhere f is the function to be tested, x are the arguments that define the conditions.\n\n\nTesting for NULL\nTo test whether, under certain conditions, the function returns NULL, the following code can be adapted, for as many test cases as considered appropriate:\ntest_that(\"function f returns NULL\", {\n  expect_null(f(x))\n})\n\n\nTesting the type of object returned (base R)\nTo test whether, the function returns an object of a certain type, the following code can be adapted, for as many test cases as considered appropriate:\ntest_that(\"function f returns object of type XXX\", {\n  expect_type(f(x), type)\n})\nWhere f is the function to be tested, x are the arguments that define the conditions and type is any of the following: “integer”, “character”, “factor”, “logical”, “double”.\n\n\nTesting the class (s3) of an object\nTo test whether, the function returns an object of class s3, the following code can be adapted, for as many cases as considered appropriate:\ntest_that(\"function f returns object of class s3\", {\n  expect_s3_class(f(x), class)\n})\nWhere f is the function to be tested, x are the arguments that define the conditions and class is, among others, any of the following: “data.frame”, “factor”, “Date”, “POSIXct”, etc.\n\n\nRunning tests under certain conditions\nOn occasion, it may be desirable to restrict the tests to specific package versions. This can be done by using the skip_if functions in testthat.\nFor example, the pivot functions we introduced to tidyr in version 1.0.0. If we have tests on those functions, we can restrict them to versions 1.0.0 and above with the following code:\nskip_if(packageVersion(\"tidyr\") &lt; \"1.0.0\")\nThis line can be placed at the top of the test file, before any tests are run. The equivalent can be done for versions below a certain version, which might be useful for deprecated functions.\nIt might be suitable to stop tests from being run if the internet is not available:\nskip_if_offline()\nOr if a package can only be run on a specific operating system:\nskip_on_os(\"mac\")",
    "crumbs": [
      "Work Intructions",
      "Writing function tests"
    ]
  },
  {
    "objectID": "wi/testing.html#submitting-tests-to-the-platform",
    "href": "wi/testing.html#submitting-tests-to-the-platform",
    "title": "Writing function tests",
    "section": "Submitting tests to the platform",
    "text": "Submitting tests to the platform\nOnce you have added the necessary tests, add the files to the validation_tests repository. The easiest way is to create a fork of the repository, then navigate to the tests folder, click on “Add file” and then “Upload files”. Now you can simply drag the pkg folder into the browser window and commit the change. You can now create a pull request to the original repository to incorporate your code. It is also possible to fork the repository, clone it to your computer and make the commit there, but this is not strictly necessary.\nAt this stage, the four-eye principle will be applied to the pull request to check the adequacy and quality of the tests and code. If the reviewer agrees with your tests, they will be merged into the package. If they note any issues, which you will see as comments in the GitHub pull request, you will need to address them before the tests can be merged.\nOnce merged, the tests can be run via the validation package validation::test(\"packagename\") and documented in the repository at https://github.com/SwissClinicalTrialOrganisation/pkg_validation.",
    "crumbs": [
      "Work Intructions",
      "Writing function tests"
    ]
  },
  {
    "objectID": "wi/testing.html#worked-example",
    "href": "wi/testing.html#worked-example",
    "title": "Writing function tests",
    "section": "Worked example",
    "text": "Worked example\nTheory is all well and good, but it’s always useful to see how that would be in practice.\nAssume that we want to check that the lm function from the stats package works as expected. We can write a test file that checks that the function returns the expected coefficients and standard errors.\nThe following assumes that we have cloned the validation_tests repository to our computer and we are within that project.\nTo begin with, we construct the testing files, specifically the test_skeleton function. We only want to test the lm function, so we only pass that to the second argument. We also specify the dir argument to tell R where to create the new files:\nThis will have created a stats folder within tests. Within that folder, there will be files called info.txt, setup-stats.R, and test-lm.R.\n\ntest-lm.R\nFirst we will write the actual tests that we want to run. Tests are entered into the test-lm.R file. Opening that file, we see that there are just a few comments at the top of the file, with some reminders.\n# Write relevent tests for the function in here\n# Consider the type of function:\n#   - is it deterministic or statistic?\n#   - is it worth checking for errors/warnings under particular conditions?\nWe decide that we will use the mtcars dataset as our basis for testing lm, so we can load the dataset. We want to test both the linear effect of the number of cylinders on the miles per gallon, and the effect of the number of cylinders (cyl), as well as when cyl is treated as a factor.\n# Write relevent tests for the function in here\n# Consider the type of function:\n#   - is it deterministic or statistic?\n#   - is it worth checking for errors/warnings under particular conditions?\n\ndata(mtcars)\nmtcars$cyl_f &lt;- factor(mtcars$cyl)\nNote that if there are multiple functions being tested (each in their own test-function.R file) that require the same data, we can load and prepare the data in the setup-stats.R file.\nWe can also define the models that we want to test:\ncmod &lt;- lm(mpg ~ cyl, data = mtcars)\nfmod &lt;- lm(mpg ~ cyl_f, data = mtcars)\nWe do not include the model definitions within a test_that call because we will use the same models in multiple tests. Again, if we needed to use those models for testing multiple functions, we could define them in the setup file.\n\nTesting coefficients\nSuppose that we know that the coefficient for mpg ~ cyl is known (-2.88 for the linear effect). We can write a test that checks that expectation:\ntest_that(\"lm returns the expected coefficients\", {\n  expect_equal(coef(cmod)[2], -2.88)\n})\nDue to floating point precision, this is probably insufficient - R will not return exactly -2.88. We can use the tolerance argument to check that the coefficient is within a certain range (we could also round the coefficient). We also need to tell expect_equal to ignore the names attribute of the vector, otherwise it compares the whole object, attributes and all:\ntest_that(\"lm returns the expected coefficients\", {\n  expect_equal(coef(cmod)[2], -2.88, tolerance = 0.01, check.attributes = FALSE)\n})\nWe can do the same for the coefficients from the model with cyl_f. This time, we can derive the values from the tapply function as, in this case, the coefficients are just the means:\ntest_that(\"lm returns the expected coefficients\", {\n  means &lt;- tapply(mtcars$mpg, mtcars$cyl, mean)\n  coefs &lt;- coef(fmod)\n  expect_equal(coefs[1], means[1], check.attributes = FALSE)\n  expect_equal(coefs[2], means[2] - means[1], check.attributes = FALSE)\n  expect_equal(coefs[3], means[3] - means[1], check.attributes = FALSE)\n})\nWe have now performed 4 tests (the expectations) in two test_that calls. We can also combine them together:\ntest_that(\"lm returns the expected coefficients\", {\n  expect_equal(coef(cmod)[2], -2.88, tolerance = 0.01, check.attributes = FALSE)\n  \n  means &lt;- tapply(mtcars$mpg, mtcars$cyl, mean)\n  coefs &lt;- coef(fmod)\n  expect_equal(coefs[1], means[1], check.attributes = FALSE)\n  expect_equal(coefs[2], means[2] - means[1], check.attributes = FALSE)\n  expect_equal(coefs[3], means[3] - means[1], check.attributes = FALSE)\n})\nWhether to put them in one or two calls is up to the author. Distributing them across more calls helps identify which tests fail, but it also makes the file longer.\n\nA note on selecting tolerances\nThe tolerance is a tricky thing to select. It is a balance between being too strict and too lenient. If the tolerance is too strict, then the test will fail when the function is working as expected. If the tolerance is too lenient, then the test will pass when the function is not working as expected.\nConsider the example above. We compared -2.88 with the coefficient which R reports to (at least) 5 decimal places. In this case, it does not make sense to use a tolerance of less than 0.01 because we only know the coefficient to two decimal places (even though we would have access to a far greater precision had we worked for it).\nGenerally speaking, values that are easy to calculate should probably have a lower tolerance. Values that are very dependent on specifics of the implementation (e.g. maximisation algorithm, etc) should probably have a higher tolerance. This is especially the case when using external software as a reference (e.g. Stata uses different defaults settings to lme4, causing differences in SEs). Simulation results, may also require a more lenient tolerance.\n\n\n\nTesting the standard errors against Stata\nSuppose that we have used Stata as a reference software for the standard errors. We include the commands used in the reference software in comments in the script, including the output and the information of the version of the reference software.\n# write.csv(mtcars, \"mtcars.csv\", row.names = FALSE)\n# reference software: Stata 17.0 (revision 2024-02-13)\n# import delimited \"mtcars.csv\"\n# regress mpg cyl\n# [output truncated for brevity]\n# ------------------------------------------------------------------------------\n#          mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n# -------------+----------------------------------------------------------------\n#          cyl |   -2.87579   .3224089    -8.92   0.000    -3.534237   -2.217343\n#        _cons |   37.88458   2.073844    18.27   0.000     33.64922    42.11993\n# ------------------------------------------------------------------------------\n# [output truncated for brevity]\n# regress mpg i.cyl\n# [output truncated for brevity]\nWe can then use the SE values from Stata in the tests, specifying a suitable tolerance (it’s pretty simple to calculate, so we can be quite stringent):\ntest_that(\"Standard errors from LM are correct\", {\n  expect_equal(summary(cmod)$coefficients[2, 2], 0.322408, \n               tolerance = 0.00001)\n  expect_equal(summary(fmod)$coefficients[2, 2], 1.558348, \n               tolerance = 0.00001)\n  expect_equal(summary(fmod)$coefficients[3, 2], 1.298623, \n               tolerance = 0.0001)\n})\n\n\nThe completed test file\nThe test file including the tests above is then:\n# Write relevent tests for the function in here\n# Consider the type of function:\n#   - is it deterministic or statistic?\n#   - is it worth checking for errors/warnings under particular conditions?\n\ndata(mtcars)\nmtcars$cyl_f &lt;- factor(mtcars$cyl)\n\ncmod &lt;- lm(mpg ~ cyl, data = mtcars)\nfmod &lt;- lm(mpg ~ cyl_f, data = mtcars)\n\ntest_that(\"lm returns the expected coefficients\", {\n  expect_equal(coef(cmod)[2], -2.88, tolerance = 0.01, check.attributes = FALSE)\n  \n  means &lt;- tapply(mtcars$mpg, mtcars$cyl, mean)\n  coefs &lt;- coef(fmod)\n  expect_equal(coefs[1], means[1], check.attributes = FALSE)\n  expect_equal(coefs[2], means[2] - means[1], check.attributes = FALSE)\n  expect_equal(coefs[3], means[3] - means[1], check.attributes = FALSE)\n})\n\n# write.csv(mtcars, \"mtcars.csv\", row.names = FALSE)\n# reference software: Stata 17.0 (revision 2024-02-13)\n# import delimited \"mtcars.csv\"\n# regress mpg cyl\n# \n#       Source |       SS           df       MS      Number of obs   =        32\n# -------------+----------------------------------   F(1, 30)        =     79.56\n#        Model |  817.712952         1  817.712952   Prob &gt; F        =    0.0000\n#     Residual |  308.334235        30  10.2778078   R-squared       =    0.7262\n# -------------+----------------------------------   Adj R-squared   =    0.7171\n#        Total |  1126.04719        31  36.3241028   Root MSE        =    3.2059\n# \n# ------------------------------------------------------------------------------\n#          mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n# -------------+----------------------------------------------------------------\n#          cyl |   -2.87579   .3224089    -8.92   0.000    -3.534237   -2.217343\n#        _cons |   37.88458   2.073844    18.27   0.000     33.64922    42.11993\n# ------------------------------------------------------------------------------\n# regress mpg i.cyl\n# \n#       Source |       SS           df       MS      Number of obs   =        32\n# -------------+----------------------------------   F(2, 29)        =     39.70\n#        Model |   824.78459         2  412.392295   Prob &gt; F        =    0.0000\n#     Residual |  301.262597        29  10.3883654   R-squared       =    0.7325\n# -------------+----------------------------------   Adj R-squared   =    0.7140\n#        Total |  1126.04719        31  36.3241028   Root MSE        =    3.2231\n# \n# ------------------------------------------------------------------------------\n#          mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n# -------------+----------------------------------------------------------------\n#          cyl |\n#           6  |  -6.920779   1.558348    -4.44   0.000    -10.10796   -3.733599\n#           8  |  -11.56364   1.298623    -8.90   0.000    -14.21962   -8.907653\n#              |\n#        _cons |   26.66364   .9718008    27.44   0.000     24.67608    28.65119\n# ------------------------------------------------------------------------------\n\ntest_that(\"Standard errors from lm are correct\", {\n  expect_equal(summary(cmod)$coefficients[2, 2], 0.322408, \n               tolerance = 0.0001)\n  expect_equal(summary(fmod)$coefficients[2, 2], 1.558348, \n               tolerance = 0.0001)\n  expect_equal(summary(fmod)$coefficients[3, 2], 1.298623, \n               tolerance = 0.0001)\n})\nFor lm, other things that might be tested include the R-squared, the F-statistic, and the p-values. Generally speaking, we might also want to test that the model is of the appropriate class (also lm in this case), or that the model has the expected number of coefficients, that the function issues warnings and/or errors at appropriate times.\n\n\n\ninfo.txt\nIt is easiest to write the info.txt file once all tests have been written. It provides a listing of what has been tested in prose form and serves as a quick overview of the tests.\nThe default text the file contains a single line:\nTests for package stats\nExtra details on the tests that we have performed should be added. In this case, we might modify it to:\nTests for package stats\n- coefficients and SEs from a unvariate model with continuous and factor predictors. SEs were checked against Stata.\nWhere tests are for/from a specific version of the package, as might be the case for newly added or deprecated functions, this should also be noted.\n\n\nsetup-stats.R\nThis file should contain the code necessary to load the package and any other packages that are required for the tests. In this case, we need the stats package and the testthat package.\nThe testthat package is always needed. In general, loading the package is necessary. As stats is a standard R package, it’s no necessary in this case.\nWe also try to leave the environment as we found it, so we detach the packages via the withr::defer function. Again, in this case, we don’t want to detach it, as it is a standard package.\nif(!require(stats)) install.packages(\"stats\")\nlibrary(stats)\nlibrary(testthat)\nwithr::defer({\n  # most of the time, we would want to detach packages, in this case we don't\n  # detach(package:stats)\n}, teardown_env())\n\n\nTesting that the tests work\nAssuming the tests are in a folder called stats, which is within our current working directory, we can run the tests with:\nvalidation::test(\"stats\", download = FALSE)\nWe specify download = FALSE because validation::test will download the files from GitHub and run those tests by default. download = FALSE tells it to use the local copy of the files instead.\nThe output should return various information on our tests and system. The first part comes from testthat itself while it runs the tests, the remainder (after “Copy and paste the following…”) provides summary information that should be copied into a github issue :\n✔ | F W  S  OK | Context\n✔ |          7 | lm                                                                          \n\n══ Results ══════════════════════════════════════════════════════════════════════════════════\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 7 ]\n## Copy and paste the following output into the indicated sections of a new issue\n\nISSUE NAME: \n[Package test]: stats version 4.3.1 \n\n### Name of the package you have validated: \nstats \n\n### What version of the package have you validated? \n4.3.1 \n\n### When was this package tested? \n2024-03-18 \n\n### What was tested? \nTests for package stats \n - coefficients and SEs from a unvariate model with continuous and factor predictors. SEs were checked against Stata. \n \n\n### Test results \nPASS \n\n### Test output:\n\n\n|file      |context |test                                 | nb| passed|skipped |error | warning|\n|:---------|:-------|:------------------------------------|--:|------:|:-------|:-----|-------:|\n|test-lm.R |lm      |lm returns the expected coefficients |  8|      4|FALSE   |FALSE |       4|\n|test-lm.R |lm      |Standard errors from lm are correct  |  3|      3|FALSE   |FALSE |       0|\n\n### SessionInfo:\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=German_Switzerland.utf8  LC_CTYPE=German_Switzerland.utf8   \n[3] LC_MONETARY=German_Switzerland.utf8 LC_NUMERIC=C                       \n[5] LC_TIME=German_Switzerland.utf8    \n\ntime zone: Europe/Zurich\ntzcode source: internal\n\nattached base packages:\n[1] graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] gh_1.4.0         validation_0.1.0 testthat_3.2.0  \n\nloaded via a namespace (and not attached):\n [1] xfun_0.40         httr2_0.2.3       htmlwidgets_1.6.2 devtools_2.4.5   \n [5] remotes_2.4.2.1   processx_3.8.2    callr_3.7.3       vctrs_0.6.5      \n [9] tools_4.3.1       ps_1.7.5          generics_0.1.3    curl_5.1.0       \n[13] tibble_3.2.1      fansi_1.0.6       pkgconfig_2.0.3   desc_1.4.2       \n[17] lifecycle_1.0.4   compiler_4.3.1    stringr_1.5.1     brio_1.1.3       \n[21] httpuv_1.6.12     htmltools_0.5.6.1 usethis_2.2.2     yaml_2.3.8       \n[25] pkgdown_2.0.7     tidyr_1.3.0       later_1.3.1       pillar_1.9.0     \n[29] crayon_1.5.2      urlchecker_1.0.1  ellipsis_0.3.2    cranlogs_2.1.1   \n[33] rsconnect_1.1.1   cachem_1.0.8      sessioninfo_1.2.2 mime_0.12        \n[37] tidyselect_1.2.0  digest_0.6.33     stringi_1.8.3     dplyr_1.1.4      \n[41] purrr_1.0.2       rprojroot_2.0.3   fastmap_1.1.1     cli_3.6.2        \n[45] magrittr_2.0.3    pkgbuild_1.4.2    utf8_1.2.4        withr_3.0.0      \n[49] waldo_0.5.1       prettyunits_1.2.0 promises_1.2.1    rappdirs_0.3.3   \n[53] roxygen2_7.3.0    rmarkdown_2.25    httr_1.4.7        gitcreds_0.1.2   \n[57] stats_4.3.1       memoise_2.0.1     shiny_1.8.0       evaluate_0.22    \n[61] knitr_1.45        miniUI_0.1.1.1    profvis_0.3.8     rlang_1.1.3      \n[65] Rcpp_1.0.11       xtable_1.8-4      glue_1.7.0        xml2_1.3.5       \n[69] pkgload_1.3.3     rstudioapi_0.15.0 jsonlite_1.8.7    R6_2.5.1         \n[73] fs_1.6.3         \n\n\n### Where is the test code located for these tests?\nplease enter manually\n\n### Where the test code is located in a git repository, add the git commit SHA\nplease enter manually, if relevant",
    "crumbs": [
      "Work Intructions",
      "Writing function tests"
    ]
  },
  {
    "objectID": "wi/testing.html#hints-for-working-with-github",
    "href": "wi/testing.html#hints-for-working-with-github",
    "title": "Writing function tests",
    "section": "Hints for working with GitHub",
    "text": "Hints for working with GitHub\nRStudio has a built-in git interface, which is a good way to manage your git repositories if you use RStudio.\nThe Happy Git and GitHub for the useR book is a comprehensive guide to working with git and GitHub. Of particular use are chapters 9 to 12 on connecting your computer with GitHub.\nThe GitHub desktop app is a good way to manage your git repositories if you are not comfortable with the command line. This is also an easy way to connect your computer with your GitHub account. There are many other GUIs for working with git repositories. See here for a listing of some of them.",
    "crumbs": [
      "Work Intructions",
      "Writing function tests"
    ]
  },
  {
    "objectID": "wi/testing.html#footnotes",
    "href": "wi/testing.html#footnotes",
    "title": "Writing function tests",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSenior Statistician, Department of Clinical Research (DCR), University of Bern↩︎\nSenior Statistical Programmer, SAKK↩︎",
    "crumbs": [
      "Work Intructions",
      "Writing function tests"
    ]
  },
  {
    "objectID": "wi/review_tests.html",
    "href": "wi/review_tests.html",
    "title": "Reviewing tests",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\nAuthored/revised by:\n\n\n\n\n\n\n\n\nName\nDate\n\n\n\n\nAlan Haynes,1\n2025-01-01\n\n\n\n\nVersion History\n\n\n\n\n\n\n\n\n\n\nVersion\nDate\nAuthor(s)\nSummary of Changes\n\n\n\n\n1.0\n2025-01-01\nAlan Haynes,\nInitial version\nOnce someone has prepared one or more tests for functions in a package and submitted them to be incorporated into the platform via a pull request. They should then be reviewed by another member of the platform, ideally from another unit, to check that the tests are programmed and documented appropriately.",
    "crumbs": [
      "Work Intructions",
      "Reviewing tests"
    ]
  },
  {
    "objectID": "wi/review_tests.html#who-reviews",
    "href": "wi/review_tests.html#who-reviews",
    "title": "Reviewing tests",
    "section": "Who reviews?",
    "text": "Who reviews?\nAll pull requests are by default automatically assigned to one individual per unit. These individuals should agree among themselves who can perform the review, potentially nominating someone else from their unit. Those that will not be performing the review can be removed from the list of assignees.\n\n\n\n\n\n\nNote\n\n\n\nIf you need a review urgently, reach out to someone by other means (e.g. email) and arrange that they perform the review for you.",
    "crumbs": [
      "Work Intructions",
      "Reviewing tests"
    ]
  },
  {
    "objectID": "wi/review_tests.html#performing-the-review",
    "href": "wi/review_tests.html#performing-the-review",
    "title": "Reviewing tests",
    "section": "Performing the review",
    "text": "Performing the review\nThe review is performed within the pull request on GitHub. There are four main tabs the pull request screen on GitHub:\n\n\n\n\n\n\nThe conversation tab is for discussions of general points about the pull request.\nThe commits tab lists the individual commits that make up the pull request. For our purposes, this is rarely of use.\nThe checks tab shows the results of automated checks that are run on the pull request. As we have no automated checks running for this repository, this tab is also not useful.\nThe files changed tab shows the changes that have been made in the pull request. This is where the review is performed.\n\nThe files changes lists all changes in all files modified during the pull request. The reviewer should look at each file in turn, checking that the changes are appropriate and that the code is well written and documented. The reviewer should also check that the tests are appropriate and that they test the correct things. Also, ensure that the details of the test in info.txt match the tests that were actually performed.\nWhere there are general points to be made, these should be made in the conversation tab. For specific points, the reviewer can comment on the specific line of code in the files tab.\n\nhover over the line to be commented on\nclick on the + that appears\ntype the comment in the box that appears\n\n\n\n\n\n\n\nclick on Start a review to submit the comment\n\nTo indicate that you have checked a particular file and that it is suitable, you can click on the Viewed box on the top right of each file.\nThe very top of the page has a box for finalising the review.\n\n\n\n\n\nIf there are no issues, the review can be marked with approve. If you have suggestions or require modifications, you can mark it as request changes. If you have questions, you can mark it as comment or request changes, whichever is most appropriate, and enter your questions in the box.",
    "crumbs": [
      "Work Intructions",
      "Reviewing tests"
    ]
  },
  {
    "objectID": "wi/review_tests.html#incorporating-the-tests-in-the-repository",
    "href": "wi/review_tests.html#incorporating-the-tests-in-the-repository",
    "title": "Reviewing tests",
    "section": "Incorporating the tests in the repository",
    "text": "Incorporating the tests in the repository\nOnce the tests have been reviewed and found to be suitable and appropriately documented, the pull request needs to be merged into the repository. Each CTU has at least one nominated individual that can perform a merge (typically the same individuals distributing reviews). This individual should check that the review has been approved and then merge the pull request using the green button at the bottom of the conversation tab, followed by the “confirm merge” button that appears afterwards.",
    "crumbs": [
      "Work Intructions",
      "Reviewing tests"
    ]
  },
  {
    "objectID": "wi/review_tests.html#footnotes",
    "href": "wi/review_tests.html#footnotes",
    "title": "Reviewing tests",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSenior Statistician, Department of Clinical Research (DCR), University of Bern↩︎",
    "crumbs": [
      "Work Intructions",
      "Reviewing tests"
    ]
  },
  {
    "objectID": "sop/sop_pkg_risk_assess.html",
    "href": "sop/sop_pkg_risk_assess.html",
    "title": "R add-on Package Risk Assessment",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\nAuthored/revised by:\n\n\n\n\n\n\n\n\nName\nDate\n\n\n\n\nMichael Coslovsky,1 Nicole Graf,2 Julien Sauser,3 Christina Huf,4 Christine Otieno,5 Elio Carreras6\n2025-01-01\n\n\n\n\nVersion History\n\n\n\n\n\n\n\n\n\n\nVersion\nDate\nAuthor(s)\nSummary of Changes\n\n\n\n\n1.0\n2025-01-01\nMichael Coslovsky, Nicole Graf, Julien Sauser, Christina Huf, Christine Otieno, Elio Carreras\nInitial version",
    "crumbs": [
      "SOPs",
      "R add-on Package Risk Assessment"
    ]
  },
  {
    "objectID": "sop/sop_pkg_risk_assess.html#the-r-package-risk-metrics-explained",
    "href": "sop/sop_pkg_risk_assess.html#the-r-package-risk-metrics-explained",
    "title": "R add-on Package Risk Assessment",
    "section": "4.1 The R package risk metrics explained",
    "text": "4.1 The R package risk metrics explained\nThe following metrics are considered when assessing an R add-on package. These are listed in Appendix-1 below. Here we provide a precise definition and, in parentheses, the name of the metric in the collection form:\n\nPackage name, version, release date: version and release date of the specific version being assessed. These are required for identifying the package and for documentation. Packages will be re-assessed upon update and version changes; documentation of older versions remains in the document. These values are not included in the risk calculation.\nPurpose (statistical_package) We define three risk levels for a package, depending on the package’s purpose and methodology:\n\n“non-statistical” packages: packages that deal only with data-wrangling and manipulation (e.g., dplyr) or with reporting processes (e.g., Sweave, xtable). Such processes are of ‘low risk’ as no statistical calculations are performed, and data-errors are, comparatively, easy to detect. Similarly, packages associated with application interfaces such as Shiny application are considered “non-statistical”.\n“Statistical with published methods”: packages that perform statistical calculations, the majority of which based on known methods, or on methods that have been published in peer reviewed journals. These packages obtain a “medium risk” status.\n“Statistical non-published methods”: packages that perform statistical calculations, but the majority of underlying methods have not been published in a peer reviewed journal. These packages obtain a status “high risk”.\n\nAuthor (author): The author(s) of a package will be viewed as indicator for its trustworthiness. If package authors (noted as ‘aut’ in the package description, e.g., as listed on CRAN) are well-known within the statistical, data-science and R communities and have credentials based on their qualifications, education, present and past affiliations, the risk of the respective package will be scored as low. If package authors have credentials based on their qualifications, education, present and past affiliations, but are not well-known within the statistical, data-science and R communities, the risk of the respective package will be scored as medium. If package authors are not well-known within the statistical, data-science and R communities and have no clear credentials based on their qualifications, education, present and past affiliations, the risk of the respective package will be scored as high.\nNote that whether an author is ‘well-known’ in the community is a subjective assessment and accepted as such; in addition, groups of authors are evaluated as a collective.\nMaintainer (maintainer): The package has a named maintainer who’s contact details (email) are available and published. A positive answer provides a “low risk” score. A package with no named maintainer is scored as high. The rationale behind this metric lies in the fact that the indication of name and email is evidence of a package’s active maintenance and availability of contact in case of bugs and/or suggestions.\nNumber of dependencies (n_dependencies): This metric assesses the level of risk associated with the number of dependencies a package relies on. Dependencies are other packages or processes that the evaluated package depends on, as listed in CRAN under “depends” and/or “imports”. The risk of unexpected behaviour increases with the number of dependencies, since there is a greater likelihood of issues on a specific routine if updates are performed on a dependent package. Great care should be considered while using packages involving many dependencies.\nThe listed number is converted into a [0, 1] score, with 0 representing low number of dependencies (= low risk) and 1 representing many dependencies (= high risk). Taking a similar approach for the transforming the number to a score as the ‘riskmetric’ package (R validation hub, 2023), we use a simplification of the classic logistic curve \\(1/\\ (1 + exp( - k\\left( x - x\\lbrack 0\\rbrack \\right))\\) as a scoring function. A sigmoid midpoint is 4 dependencies, ie., x[0] = 4 and logistic growth rate of k = 0.5.\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nCRAN or Bioconductor (on_cran): CRAN and Bioconductor impose a set of requirements that a package has to meet for it to be released on their official platform. Thus, they provide a procedure of evaluating a minimal quality standard for a technical, though not statistical, appropriateness of the package. This metric assesses whether the package is on CRAN or Bioconductor. Being on CRAN/Bioconductor provides low risk (yes = 0) while not being on them represents high risk (no = 1).\nDocumentation of source code (source_code_documented): Ideally, the source code is available (for example, on github) for examination and commented (yes = 0). Source code that is not commented or difficult to follow, or is not available, is considered not documented (no = 1).\nNumber of downloads in the last year (nr_downloads_12_months): More downloads suggest more extensive community and user testing and greater chances of bugs or errors being identified and reported. To fill in check the logs of CRAN’s or Bioconductor’s reporting systems and report the number of downloads for the package in the last 12 months. Using the cranlogs::cran_downloads() function to this end is acceptable.\nThe number of downloaded packages is converted to a score [0,1], with 0 representing low risk (many downloads) and 1 high risk (few downloads). For the conversion of the number into a score we follow a similar approach as approach taken by the ‘riskmetric’ package (R validation hub, 2023) and use a simplification of the classic logistic curve \\(1/\\ (1 + exp( - k\\left( x - x\\lbrack 0\\rbrack \\right))\\) with the logistic growth rate k = -0.5 and a log-scale for the number of downloads (log(x)). The midpoint lying at log(100,000 downloads).\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\nBug reporting (bug_reporting_active): Available option for reporting bugs suggests higher chance that errors have been corrected. Check whether there is an option to report bugs – ideally via url or email. Yes = 0 (low risk); no = 1 (high risk).\nVignettes (has_vignettes): Does the package have one/more available vignettes? Vignettes provide an explanation of the use of the package, increasing its trustworthiness and correct use. Having at least one vignette suggests a “low” risk for this category (yes = 0); having no vignettes suggests a “high” risk score (no = 1).\nTested functions (has_tests): Perform a search and give a general grade of low/medium/high based on the answers to the following questions: does the package have unit and/or function tests performed by the authors? are they comprehensive? are they well documented? Assess the above to determine whether tests were conducted sufficiently and documented. Accordingly, you can classify into low risk (=0); insufficient testing could be medium risk (=0.5); no documented testing at all are categorized as high risk (=1).\nNote that test functions are often listed in ‘test’ folder in the package’s source files, e.g., on github.",
    "crumbs": [
      "SOPs",
      "R add-on Package Risk Assessment"
    ]
  },
  {
    "objectID": "sop/sop_pkg_risk_assess.html#calculation-of-the-final-risk-score",
    "href": "sop/sop_pkg_risk_assess.html#calculation-of-the-final-risk-score",
    "title": "R add-on Package Risk Assessment",
    "section": "4.2 Calculation of the final risk score",
    "text": "4.2 Calculation of the final risk score\nThe final score is a weighted summary of all the measures above in the range [0, 1], with lower scores representing lower risk and higher scores higher risk. In this version of the guideline all measures are considered equally important, and the score is a simple arithmetic mean of the measures.\nThe [0, 1] score is then categorized to low, medium and high-risk R add-on packages:\n\nScore ≤0.25: low risk\n0.25 &lt; score ≤ 0.75: medium risk\nScore &gt;0.75: high risk\n\nAs described in the SCTO computerized systems validation policy for R, the risk associated with the R add-on package is evaluated alongside the risk associated with a specific project to determine which actions may be required to use the package. High risk R add-on packages, or medium risk packages in high-risk projects, for example, may need (project-) specific function testing.",
    "crumbs": [
      "SOPs",
      "R add-on Package Risk Assessment"
    ]
  },
  {
    "objectID": "sop/sop_pkg_risk_assess.html#documentation",
    "href": "sop/sop_pkg_risk_assess.html#documentation",
    "title": "R add-on Package Risk Assessment",
    "section": "4.3 Documentation",
    "text": "4.3 Documentation\nThe assessment of risk associated with an R add-on package should be documented and may be updated with time.\nThe SCTO has developed a platform on GitHub, on which the risk assessment can be performed (https://github.com/SwissClinicalTrialOrganisation/validation_platform). R add-on package assessments are done as “issues” on the GitHub platform (https://github.com/SwissClinicalTrialOrganisation/validation_platform/issues).\nThe platform allows performing the assessment and calculating the final risk based on the above metrics, while recording and documenting the assessment. Apart from the metrics themselves (Section 6), the additional points listed in (Section 7) are collected and documented while performing the assessment on the platform.",
    "crumbs": [
      "SOPs",
      "R add-on Package Risk Assessment"
    ]
  },
  {
    "objectID": "sop/sop_pkg_risk_assess.html#footnotes",
    "href": "sop/sop_pkg_risk_assess.html#footnotes",
    "title": "R add-on Package Risk Assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHead data-analysis, Department Clinical Research, University of Basel↩︎\nStatistician, Clinical Trials Unit, Kantonspital St. Gallen↩︎\nStatistician, CHUV↩︎\nHead Quality Management, Department Clinical Research, University of Bern↩︎\nIT Quality Manager, Department Clinical Research, University of Basel↩︎\nSenior Statistical Programmer, SAKK↩︎",
    "crumbs": [
      "SOPs",
      "R add-on Package Risk Assessment"
    ]
  },
  {
    "objectID": "sop/businessproc_ra.html",
    "href": "sop/businessproc_ra.html",
    "title": "Business Processes Risk Assessment",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\nAuthored/revised by:\n\n\n\n\n\n\n\n\nName\nDate\n\n\n\n\nMichael Coslovsky1\n2025-01-01\n\n\n\n\nVersion History\n\n\n\n\n\n\n\n\n\n\nVersion\nDate\nAuthor(s)\nSummary of Changes\n\n\n\n\n1.0\n2025-01-01\nMichael Coslovsky\nInitial version\nThis is an explanation and ‘dictionary’ for the Business Processes Risk Assessment tool of the platform.\nThe file is structured as a process following the steps below:\nThe variables of decision are:\nThe business processes risk assessment is available here .",
    "crumbs": [
      "Business Processes Risk Assessment"
    ]
  },
  {
    "objectID": "sop/businessproc_ra.html#footnotes",
    "href": "sop/businessproc_ra.html#footnotes",
    "title": "Business Processes Risk Assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHead data-analysis, Department Clinical Research, University of Basel↩︎",
    "crumbs": [
      "Business Processes Risk Assessment"
    ]
  },
  {
    "objectID": "results/pkgassessment.html",
    "href": "results/pkgassessment.html",
    "title": "Package assessments",
    "section": "",
    "text": "Note\n\n\n\nThis page currently requires manually rebuilding the site to show the newest packages. It was last built at 2025-07-01 13:47:21.\n\n\n73 packages or package versions have been risk assessed within the SCTO Statistics & Methodology Platform so far.\n\n\n\n\n\n\n\n\n\n\nPackages with potentially outdated risk assessments:",
    "crumbs": [
      "Package assessments"
    ]
  },
  {
    "objectID": "acknowledge.html",
    "href": "acknowledge.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "Creating this validation platform has been a team effort, including representatives from SCTO affiliated units across Switzerland. There are many people that have contributed to its development, beyond those mentioned in the author lists of the individual documents, be it by testing, reviewing, or discussions. We would like to thank all of them for their contributions.\nAlan Haynes, Alfonso Rojas, André Moser, Arnaud Künzi, Christina Huf, Christine Otieno, Christoph Combescure, Constantin Sluka, Cyril Jaksic, David Krontaler, Elio Carreras, Erin West, Hoda Mazaheri, Julien Sauser, Marie Roumet, Mattia Branca, Michael Coslovsky, Mirelle Moser, Monika Hebeisen, Nicole Graf, Odile Stalder, Ramon Saccilotto, Roland John, Stefanie von Felten, Synove Otterbech, Ulrike Held, and many others.\nFurthermore, this work builds off the ideas of various other groups, such as:\n\nPHUSE’s R Package Validation Framework\nThe R Validation Hub\nA paper at PharmaSUG 2018 from Peter Schaefer and Debra Fontana\n\nFinally, we thank you for using this platform and we hope it will be of great help to you in your work.",
    "crumbs": [
      "Acknowledgments"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SCTO Computerized Systems Validation Policy for R",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\n\n\n\n\n\n\n\n\nName\nDate\n\n\n\n\nMichael Coslovsky,1 Christine Otieno,2 Alan Haynes3\n2025-01-01\n\n\n\n\nVersion History\n\n\n\n\n\n\n\n\n\n\nVersion\nDate\nAuthor\nSummary of Changes\n\n\n\n\n1.0\n2025-01-01\nMichael Coslovsky, Christine Otieno, Alan Haynes\nInitial draft",
    "crumbs": [
      "SCTO Computerized Systems Validation Policy for R"
    ]
  },
  {
    "objectID": "index.html#sec-in-scope",
    "href": "index.html#sec-in-scope",
    "title": "SCTO Computerized Systems Validation Policy for R",
    "section": "6.1 In Scope",
    "text": "6.1 In Scope\n\nRecommendations for R base installation validation and periodic review of the R base installation validation documentation set (Section 7.2).\n\n\nNote: The final validation documentation set and periodic review have to be defined and prepared by each CTU according to their local processes.\n\n\nProcesses for R add-on packages validation (\"intended for use”) , incl. R add-on package risk assessment & functional testing as well as periodic review (Section 7.3).\n\n\nNote: Processes for R add-on package validation described in this document are to be considered minimum requirements as shared by the SCTO platform. Any CTU providing input to the SCTO’s R add-on package validation repository has to follow at least those processes. Individual organizations (CTUs) can define stricter processes.\n\n\nRecommendations for user training (Section 9).",
    "crumbs": [
      "SCTO Computerized Systems Validation Policy for R"
    ]
  },
  {
    "objectID": "index.html#sec-out-of-scope",
    "href": "index.html#sec-out-of-scope",
    "title": "SCTO Computerized Systems Validation Policy for R",
    "section": "6.2 Out of Scope",
    "text": "6.2 Out of Scope\n\nStandards for IT infrastructure (*to be defined on local level by the CTUs, see Section 7.1)\nR add-on package management to ensure traceability and reproducibility on R product level (to be defined on local level by the CTUs, see Section 7.3)\nRisk assessment and management of R products (to be defined on local level by the CTUs, see Section 7.4: R Products are always CTU project specific (e.g., a CTU’s clinical trial project) – a high-risk R product may result in the need for additional validation activities for an R add-on package, even if that R add-on package/required function is available as “validated” on the SCTO platform.\nInternal, organization specific, processes of CTUs.",
    "crumbs": [
      "SCTO Computerized Systems Validation Policy for R"
    ]
  },
  {
    "objectID": "index.html#sec-it-iq",
    "href": "index.html#sec-it-iq",
    "title": "SCTO Computerized Systems Validation Policy for R",
    "section": "7.1 IT Infrastructure Level: IT Infrastructure Qualification",
    "text": "7.1 IT Infrastructure Level: IT Infrastructure Qualification\nThe IT infrastructure qualification is managed at a local, institutional level, by the CTU or its organizational entity according to local IT processes.\nFor the reliable use of R and statistical traceability it is important to note which IT infrastructure (specifically which operating system software and version) R is installed on and to ensure that the infrastructure on which R is running is qualified.",
    "crumbs": [
      "SCTO Computerized Systems Validation Policy for R"
    ]
  },
  {
    "objectID": "index.html#sec-r-base-install",
    "href": "index.html#sec-r-base-install",
    "title": "SCTO Computerized Systems Validation Policy for R",
    "section": "7.2 R System Level: R Base Installation Validation",
    "text": "7.2 R System Level: R Base Installation Validation\nWe recommend, that every CTU defines:\n\nThe minimal (validated) software version of the R Base Installation to be used at the CTU and a process of handling version changes.\nThe R base installation validation documentation set, providing evidence for the R base installation validation activities (see Table 3for recommendations).\n\nTable 2 lists all documentation that is typically prepared during a computerised system validation process according to global standards and guidelines. It additionally provides recommendations how this documentation may be covered when preparing an R base installation validation documentation set. The exact content of the R base installation validation documentation set (i.e., the required documentation) at a CTU should follow the local processes and all documentation must be prepared and approved by the CTU. For some of the documents listed below, the SCTO provides examples (as indicated under “Detailed Description”).\n\n\n\nTable 2: R Base Installation Validation: Detailed Description of Required Documents and Activities\n\n\n\n\n\nDocument or activity\nR Base installation\nDetailed Description:\nRequirements\n\n\n\n\n\n\nPurpose of the document/ activity and how this aspect can be covered for R (suggested R specific document/activity)\nLink to example document\n\n\nIf the document/ activity is NOT required for R: a rationale, why this document/activity can be omitted\nRecommended frequency of update\n\n\n\nR Base installation\nHigh-Level Risk Assessment (HLRA; also referred to as System Risk Assessment)\nPurpose:\nThe purpose of such a document is to assess and document the risk of a system on a high level. It typically specifies at least the software category and the intended use of the software, including if it is used for GxP processes. The outcome of the HLRA (GxP relevance assessment) defines, if a system needs to be validated or not.\nExample:\n\nExample\n\nRequired for R base installation validation:\n\nYes\n\nFrequency of update:\n\nCreated before/ with the first R base installation validation.\nUpdated, if any of the assessed aspects change.\nReviewed during periodic review cycles. The review process should be documented even if no updates are required.\n\n\n\nR Base installation\nVendor Assessment\nPurpose:\nThe purpose of such a document is to the assess reliability of a system vendor. This may be done based on an audit or review of relevant vendor documentation.\nR base validation specific approach: Since R is an open-source software managed by a consortium, a traditional vendor assessment/ audit approach is not feasible. A critical review of “R: Regulatory Compliance and Validation Issues - A Guidance Document for the Use of R in Regulated Clinical Trial Environments” may suffice as vendor assessment and should be documented together with any gaps identified that may require attention during the validation of R and its R add-on packages.\nExample:\n\nExample\n\nRequired for R base installation validation:\n\nYes\n\nFrequency of update:\n\nCreated before/ with the first R base installation validation.\nUpdated, if any of the assessed aspects change, specifically, with a publication of a new article version of the article mentioned above, or in case the R consortium replaces the existing document.\nReviewed during periodic review cycles and review documented, if no updates are required.\n\n\n\nR Base installation\nValidation Plan & Test Plan\nPurpose:\nThe purpose of such a document is to define the details of the planned validation process, including required documentation and acceptance criteria for productive use (Validation Plan). The Test Plan may be included in the Validation Plan or prepared as a separate document. The purpose of a Test Plan is to define the testing strategy and testing process and may contain details on the tests to be executed (if created for every new version validated). The exact content of such a document depends on the CTU’s local processes.\nExample:\n\nExample\n\nRequired for R base installation validation:\n\nYes\n\nFrequency of update:\n\nBased on each CTU’s validation approach, either:\n\nCreated with the first R base installation validation and updated only, if any aspects described in the plan change (in this case the plan would NOT contain any details about tests to be executed or specific document updates required for a newly released version. Instead, it would contain generic rules for test execution and document updates). If this approach is followed:\n\nRelease-specific validation activities should be documented elsewhere (e.g., in a Change Plan)\n\nThe document is only updated, if the validation and/or testing strategy changes.\nThe document is reviewed during periodic review cycles and review documented, if no updates are required.\n\n\nCreated or at least updated to a new document version with every validation of a new R base installation version (in this case the plan would contain all relevant details to plan and execute the release-specific validation). If this approach is followed\n\nAn additional release-specific change plan may not be required.\nA new Validation & Test Plan or at least a new document version is created with every new R base installation version validation.\nNo periodic review required.\n\n\n\n\n\nR Base installation\n(User) Requirements Specification\nPurpose:\nThe purpose of such a document is to specify the user requirements and intended use for the R base installation. It may also contain any compliance (e.g., data integrity), regulatory (e.g., personal data protection) and safety requirements (e.g., controlled access).\nExample:\n\nExample\n\nRequired for R base installation validation:\n\nYes\n\nFrequency of update:\n\nCreated with the first R base installation validation.\nUpdated, if any new requirements need to be added to cover the full scope of the current intended use OR if any of the existing requirements need to be updated or removed.\nReviewed during periodic review cycles and review documented, if no updates are required.\n\n\n\nR Base installation\n(Functional) Risk Assessment\nPurpose:\nThe purpose of such a document is to document the risk of system functionality based on the user requirements and/ or processes related to these requirements.\nExample:\n\nYou may use the SCTO Statistics Business processes risk assessment document as a basis for this document\n\nRequired for R base installation validation:\n\nYes\n\nFrequency of update:\n\nCreated with the first R base installation validation.\nUpdated, if any new functions or processes are added to the scope of the intended use of R. If new requirements are added to the User Requirements Specification, this is a good indicator to also review the Functional Risk Assessment document.\nReviewed during periodic review cycles and review documented, if no updates are required.\n\n\n\nR Base installation\nSoftware Installation Plan/Instruction\nPurpose:\nThe purpose of such a document would be to provide a process to follow during the installation of a software. It is typically provided either by the vendor or prepared by local IT departments.\nR base validation specific approach: It should be sufficient to follow the instructions on CRAN when downloading and installing the new R base installation version.\nImportant is that the installation of the R base installation and version management follow the CTU’s local processes for validated systems and that all versions installed are documented, including the date/time of installation (see Installation Verification Document below).\nRequired for R base installation validation:\n\nNo (see above for how this is covered)\n\nFrequency of update:\n\nN/A\n\n\n\nR Base installation\nTest protocol (“User Acceptance Tests”)\nPurpose:\nThe purpose of such a document is to verify the functions of the R base installation supporting the user requirements.\nR base validation specific approach: Test protocols provided by R may be used as-is or adapted/ extended as required. The scope of testing must be defined according to the CTU’s local processes and defined user requirements.\nExample:\n\nAn example will be available soon\n\nRequired for R base installation validation:\n\nYes\n\nFrequency of update:\n\nCreated with the first R base installation validation.\nUpdated, if any new functions or processes are added to the scope of the intended use of R. If new requirements are added to the User Requirements Specification, this is a good indicator to also review the existing test protocols.\nReviewed during periodic review cycles and review documented, if no updates are required.\n\n\n\nR Base installation\nSoftware Installation Verification document (TEST)\nPurpose:\nThe purpose of such a document is to provide evidence of the installation of a specific system version on an environment/ machine that is used for testing purposes before the new system version is released to production.\nR base validation specific approach: In case of R this may be a separate document or an entry in an R repository or other “version tracker” (depending on your local IT processes). The following information should be captured:\n\nR base installation version installed\nDate and time of installation\nPerson performing the installation (may be someone from your local IT IT department)\nOutcome of installation: successful, successful with deviations, not successful\nIf applicable: Any actions taken in addition to the regular “download and install from CRAN” process (e.g., uninstalling an older version)\n\nNote: depending on your local processes it may not be feasible or even possible to do a preliminary “test installation” and perform tests before productive use of the new R base installation version (e.g., if R is distributed remotely by your local IT department). In this case, we recommend you still execute the tests after the installation and document the test execution. Should any issues be detected during testing, please inform your local IT department immediately. Ideally you implement a process that would ensure nobody is using the new R version for productive use before the tests are completed successfully (i.e., without major issues) and the validation & test report is approved (see “Validation & Test Report” below).\nExample:\n\nsee Example for Validation & Test Report available\n\nRequired for R base installation validation:\n\nYes\n\nFrequency of update:\n\nTo be created with every new R base installation version to be validated for productive use\nNo periodic review required\n\n\n\nR Base installation\nExecuted test protocols (“User Acceptance Tests”)\nPurpose:\nThe purpose of such a document is to provide evidence for the execution of the pre-defined test protocols for each new R base installation version (i.e., follow the steps described in the test protocols and document the outcome according to the CTU’s local processes).\nExample:\n\nAn example will be available soon\n\nRequired for R base installation validation:\n\nYes\n\nFrequency of update:\n\nTo be created with every validation of a new R base installation version.\nNo periodic review required\n\n\n\nR Base installation\nTraceability Matrix\nPurpose:\nThe purpose of such a document is to ensure and provide evidence that all user requirements are tested during the validation process. You may omit testing low risk requirements, if such an approach is described in the Validation & Test Plan and complies to your CTU’s processes.\nTraceability between requirements and tests may be achieved by creating a separate document (traceability matrix) or by linking requirements and tests within the requirements and tests themselves. It is important, that whatever means is used, it is feasible to proof that all requirements are verified with a test (or if not tested formally, a rationale why the test is not required, e.g., low risk requirements which can be considered “verified” based on the experience by the user community).\nExample:\n\nAn example will be available soon\n\nRequired for R base installation validation:\n\nYes\n\nFrequency of update:\n\nCreated with the first R base installation validation.\nUpdated, if the user requirements specification and or the test protocols are updated.\nReviewed during periodic review cycles and review documented, if no updates are required.\n\n\n\nR Base installation\nValidation & Test Report\nPurpose:\nThe purpose of such a document is to\n\nsummarize the testing activities and results, including any deviations from the expected results\ndocument all completed validation activities, including any deviations from the original plan (see Validation & Test Plan). Deviations should be justified and assessed for criticality.\ndocument the acceptance for productive use (with or without restrictions)\n\nThe exact content of this report depends on the CTU’s local processes. Separate documents may be created to summarize the testing activities and results (Test Report) and the overall executed validation against the plan, incl. a statement of acceptance of an R base installation version for productive use (Validation Report).\nExample:\n\nExample\n\nRequired for R base installation validation:\n\nYes\n\nFrequency of update:\n\nTo be created with every new R base installation version validated for productive use\nNo periodic review required\n\n\n\nR Base installation\nSoftware Installation Verification document (PROD)\nPurpose:\nThe purpose of such a document is to provide evidence of the installation of a specific system version on an environment/ machine that is used for productive purposes.\nR base validation specific approach: In case of R, this may be a separate document or an entry in an R repository or other “version tracker” (depending on your local IT processes). The following information should be captured:\n\nR base installation version installed\nDate and time of installation\nPerson performing the installation (may be someone from your local IT department)\nOutcome of installation: successful, successful with deviations, not successful\nIf applicable: Any actions taken in addition to the regular “download and install from CRAN” process (e.g., uninstalling an older version)\n\nSee also note under “Installation Verification document (TEST)”: In case of R base installation, the installation may only be documented once and not separately on “TEST” and “PROD”. Important is, that any version changes of the R base installation are managed and documented according to your CTU’s local IT change management processes.\nRequired for R base installation validation:\n\nYes\n\nFrequency of update:\n\nTo be created with every new R base installation version validated for productive use\nNo periodic review required",
    "crumbs": [
      "SCTO Computerized Systems Validation Policy for R"
    ]
  },
  {
    "objectID": "index.html#sec-r-package-val",
    "href": "index.html#sec-r-package-val",
    "title": "SCTO Computerized Systems Validation Policy for R",
    "section": "7.3 R Package Level: R Add-on Package Validation",
    "text": "7.3 R Package Level: R Add-on Package Validation\nThe R add-on package validation documentation set provides evidence of the validation process of an “intended for use” R add-on package following this policy and associated SOPs. Table 3 lists all documentation that is typically prepared during a computerised system validation process. It additionally provides details on how this documentation is covered for an R add-on package validation according to this policy.\nNOTE on dependencies: R add-on packages often come with ‘dependencies’, namely additional add-on packages required for the proper utilization of the specified R add-on package. Package dependency ‘trees’ can be very large and complex, raising the challenge of validation of the dependencies as well. Validating at a single step a specific R add-on package with all its dependencies is thus almost unfeasible. Hence, in this policy, when referring to the validation of an add-on package, the meaning is the “intended for use” R add-on package itself and its main functions, leaving dependencies unvalidated.\n\n\n\nTable 3: R add-on Package Validation: Detailed Description of Required Documents and Activities\n\n\n\n\n\nDocument or activity\nR add-on package\nDetailed Description:\nRequirements\n\n\n\n\n\n\nPurpose of the document/ activity and how this aspect can be covered for R (suggested R specific document/ activity)\nLink to example/relevant SCTO document\n\n\nIf the document/ activity is NOT required for R: a rationale, why this document/ activity can be omitted\nRecommended frequency of update\n\n\n\nR add-on Package\nHigh-Level Risk Assessment (HLRA; also referred to as System Risk Assessment)\nPurpose:\nSee Table 3: R Base Installation Validation: Detailed Description of Required Documents and Activities.\nR add-on package specific approach: The R add-on package risk depends on factors described in the SCTO R add-on Package Risk Assessment SOP. Accordingly, the R add-on package high-level risk assessment should be documented according to the SCTO R Package Risk Assessment SOP on the SCTO designated platform.\nRelevant SCTO SOP:\n\nSCTO R add-on Package Risk Assessment SOP\n\nRequired for R add-on package validation:\n\nYes (see above for how this is covered)\n\nFrequency of update:\n\nSee Section 10\n\n\n\nR add-on Package\nVendor Assessment\nPurpose:\nSee Table 3: R Base Installation Validation: Detailed Description of Required Documents and Activities .\nR add-on package specific approach: Covered by the Vendor Assessment created for the R Base Installation and the SCTO R add-on Package Risk Assessment SOP.\nRelevant SCTO SOP:\n\nSCTO R Validation Policy (this document)\nSCTO R add-on Package Risk Assessment SOP\n\nRequired for R add-on package validation:\n\nNo (see above for how this is covered)\n\nFrequency of update:\n\nSee Section 10\n\n\n\nR add-on Package\nValidation & Test Plan\nPurpose:\nThe purpose of such a document would be to define the details of the planned validation process, including required documentation and acceptance criteria for productive use (Validation Plan). The Test Plan may be included in the Validation Plan or prepared as a separate document. The purpose of a Test Plan is to define the testing strategy and testing process and may contain details on the tests to be executed (if created for every new version validated).\nR add-on package specific approach: The content typically contained in a Validation & Test Plan is determined, with respect to R add-on packages, in the various steps of this policy and the associated SOPs and its documentation is specified therein as follows:\n\nDetermine the R product associated risk (local CTU SOP)\nDetermine the R add-on package associated risk (based on SCTO R Package Risk Assessment SOP)\nDetermine, if testing is required (see Table 5: Assessment of Combined Risk R add-on Package and R Product)\nPerform and document testing, if required (SCTO R add-on Package Function Testing SOP)\nDocument compliance with this policy and above-mentioned SOPs (local CTU process, e.g., in metadata or in a document)\n\nRelevant SCTO Documentation:\n\nSCTO R Validation Policy (this document)\nSCTO R add-on Package Risk Assessment SOP\nSCTO R add-on Package Function Testing SOP\n\nRequired for R add-on package validation:\n\nNo (see above for how this is covered)\n\nFrequency of update:\n\nSee Section 10\n\n\n\nR add-on Package\n(User) Requirements Specification\nPurpose:\nSee Table 3: R Base Installation Validation: Detailed Description of Required Documents and Activities.\nR add-on package specific approach: The requirements and intended use of an R add-on package and its functions can only be defined in relation to an R product and the required outcome (“intended use”) of a specific function for that R product. Therefore, the requirements can only be documented when planning the R product.\nRequirements of the product, which guide the selection of R add-on packages and functions therein, shall be documented in the Statistical Analysis Plan, a Statistical Protocolor other, similar documentation9.\nRelevant SCTO Documentation:\n\nN/A (processes for planning R products and defining their intended use have to be defined on CTU level)\n\nRequired for R add-on package validation:\n\nNo (see above for how this is covered)\n\nFrequency of update:\n\nUpon changes or deviations from the originally intended use/ requirements defined in the plan for an R product change.\n\n\n\nR add-on Package\nFunction Risk Assessment\nPurpose:\nThe purpose of such a document would be to specify the risks related to a specific function.\nR add-on package specific approach: We take a package level approach - the assessed baseline risk of the R add-on package defines the risk of all functions included within.\nRelevant SCTO SOP: - SCTO R add-on Package Risk Assessment SOP\nRequired for R add-on package validation:\n\nNo (covered by the R add-on Package High-level Risk assessment)\n\nFrequency of update:\n\nNA\n\n\n\nR add-on Package\nSoftware Installation Plan/Instruction\nPurpose:\nSee Table 3: R Base Installation Validation: Detailed Description of Required Documents and Activities.\nR add-on package specific approach: The installation follows a standard procedure using R base installation functions.\n\nR add-on packages may be installed for functional testing after the high-level R add-on package risk and the R product related requirements and functional risk are documented and the testing is planned, if the initial assessment shows that the required R add-on package is not yet sufficiently tested by one of the SCTO members providing input to the R add-on package validation repository.\nR add-on packages may be installed for productive use/ used to create R products after the functional tests are completed and no deviations were detected that would raise concerns against using the tested R add-on package functions for a given R product OR if the initial assessment shows that the required R add-on package was already sufficiently tested and the respective validation documentation set is available in the SCTO R add-on package validation repository.\n\nRelevant SCTO Documentation:\n\nN/A (standard R process, CTU specific IT operations SOPs may exist to cover local specifics of installation)\n\nRequired for R add-on package validation:\n\nNo (see above for how this is covered)\n\nFrequency of update:\n\nN/A for standard R process\nAccording to CTU-specific periodic review cycle\n\n\n\nR add-on Package\nTest scripts\nPurpose:\nThe purpose of such a document is to verify the functions of the R add-on package work as expected.\nR add-on package specific approach: The process of writing and executing tests for the R add-on packages is covered in the SCTO R add-on Package Function Testing SOP.\nRelevant SCTO SOP:\n\nSCTO R add-on Package Function Testing SOP\n\nRequired for R add-on package validation:\n\nYes (see above for how this is covered)\n\nFrequency of update:\n\nSee SCTO R add-on Package Function Testing SOP (check of appropriateness and sufficiency of an existing test script)\n\n\n\nR add-on Package\nExecuted Software installation verification (TEST)\nPurpose:\nThe purpose of such a document would be to provide evidence of the installation of a specific R add-on package (version) for testing purposes.\nR add-on package specific approach: In the context of R add-on package validation it is not feasible to create a separate document. The tested R add-on package (version) functions are documented when following the SCTO R add-on Package Function Testing SOP.\nRelevant SCTO SOP:\n\nSCTO R add-on Package Function Testing SOP\n\nRequired for R add-on package validation:\n\nNo (see above for how this is covered)\n\nFrequency of update:\n\nNA\n\n\n\nR add-on Package\nExecuted Test Scripts\nPurpose:\nThe purpose of such a document is to provide evidence for the execution of the pre-defined test scripts.\nR add-on package specific approach: Execute the pre-defined test scripts and document the outcome of each step as well as the overall outcome of the test script according to SCTO R add-on Package Function Testing SOP.\nRelevant SCTO SOP:\n\nSCTO R add-on Package Function Testing SOP\n\nRequired for R add-on package validation:\n\nYes (see above for how this is covered)\n\nFrequency of update:\n\nN/A (test executions are never updated. Test scripts may be re-executed, if required)\n\n\n\nR add-on Package\nTraceability Matrix\nPurpose:\nThe purpose of such a document is to ensure and provide evidence that all functions in scope are tested during the validation process. You may omit testing low risk functions.\nR add-on package specific approach: Covered by SCTO R add-on Package Risk Assessment SOP and SCTO R add-on Package Function Test SOP.\nRelevant SCTO SOP:\n\nSCTO R add-on Package Risk Assessment SOP\nSCTO R add-on Package Function Testing SOP\n\nRequired for R add-on package validation:\n\nYes (Documentation of function testing according to SCTO R add-on Package Function Test SOP includes info on the tested function)\n\nFrequency of update:\n\nDepends on the update and execution of tests for specific functions.\n\n\n\nR add-on Package\nValidation & Test Report\nPurpose:\nThe purpose of such a document is to summarise all executed validation activities and their outcome, including testing and any defects found and/or any deviations from the original plan with a rationale. It documents the acceptance for productive use (with or without restrictions) of specific functions from an R add-on package (version) (see SCTO R add-on Package Function Testing SOP).\nRelevant SCTO SOP:\n\nSCTO R add-on Package Function Testing SOP\n\nRequired for R add-on package validation:\n\nYes\n\nFrequency of update:\n\nSee process described in SCTO R add-on Package Function Testing SOP",
    "crumbs": [
      "SCTO Computerized Systems Validation Policy for R"
    ]
  },
  {
    "objectID": "index.html#sec-r-product-val",
    "href": "index.html#sec-r-product-val",
    "title": "SCTO Computerized Systems Validation Policy for R",
    "section": "7.4 R Product Level: R Product Validation",
    "text": "7.4 R Product Level: R Product Validation\nTo be covered by local SOPs for statistical analysis execution at the CTUs. The R product validation should cover at minimum:\n\nAssessment of risk associated with the R product\nAssess the tools used within the R product\nDocumentation for reproducibility and traceability",
    "crumbs": [
      "SCTO Computerized Systems Validation Policy for R"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "SCTO Computerized Systems Validation Policy for R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHead data-analysis, Department Clinical Research, University of Basel↩︎\nIT Quality Manager, Department Clinical Research, University of Basel↩︎\nSenior Statistician, Department of Clinical Research (DCR), University of Bern↩︎\nfor purposes of this document referred to as CTUs = Clinical Trial Units↩︎\nA validation documentation set is prepared to provide evidence of the validation process.↩︎\nGxP processes refer to Good Practice standards, including Good Clinical Practice (GCP), Good Manufacturing Practices (GMP), Good Laboratory Practice (GLP) etc. GxP critical computerised systems are all systems that manage GxP data (e.g., clinical study data) and therefore support and/or provide input for GxP processes.↩︎\nThe GNU Project is a free software, mass collaboration project announced by Richard Stallman on September 27, 1983. Its goal is to give computer users freedom and control in their use of their computers and computing devices by collaboratively developing and publishing software that gives everyone the rights to freely run the software, copy and distribute it, study it, and modify it. GNU software grants these rights in its license (https://en.wikipedia.org/wiki/GNU_Project).↩︎\nFor example, standards set by the local IT department.↩︎\nExamples of such other documents could be the defined research question when planning a sample-size/power estimation, description of data-base/shiny-app user requirements, etc.↩︎",
    "crumbs": [
      "SCTO Computerized Systems Validation Policy for R"
    ]
  },
  {
    "objectID": "results/pkgtesting.html",
    "href": "results/pkgtesting.html",
    "title": "Package tests",
    "section": "",
    "text": "15 packages or package versions have been tested by the SCTO Statistics & Methodology Platform so far.",
    "crumbs": [
      "Package tests"
    ]
  },
  {
    "objectID": "sop/sop_pkg_fn_testing.html",
    "href": "sop/sop_pkg_fn_testing.html",
    "title": "R add-on Package Function testing",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\nAuthored/revised by:\n\n\n\n\n\n\n\n\nName\nDate\n\n\n\n\nMichael Coslovsky,1 Alan Haynes,2 Lisa Hofer,3 Christina Huf,4 Christine Otieno,5 Elio Carreras,6\n2025-01-01\n\n\n\n\nVersion History\n\n\n\n\n\n\n\n\n\n\nVersion\nDate\nAuthor(s)\nSummary of Changes\n\n\n\n\n1.0\n2025-01-01\nMichael Coslovsky, Alan Haynes, Lisa Hofer, Christina Huf, Christine Otieno, Elio Carreras\nInitial version",
    "crumbs": [
      "SOPs",
      "R add-on Package Function testing"
    ]
  },
  {
    "objectID": "sop/sop_pkg_fn_testing.html#working-instructions",
    "href": "sop/sop_pkg_fn_testing.html#working-instructions",
    "title": "R add-on Package Function testing",
    "section": "5.1 Working instructions",
    "text": "5.1 Working instructions\n\n\n\n\n\n\nSuggestion from Alan\n\n\n\n(to replace the paragraph just below, which is basically the same thing twice…)\nDetailed working instructions for writing tests, reviewing tests, and running tests can be found in the respective work instructions. A summary of the main steps is provided below.\n\n\nThe working instructions for the unit testing process are found under the relevant work instruction. The detailed working instructions can be found on the GitHub page; Below we provide only a bullet-point description of the relevant steps:\n\nUnit tests of an R function are performed and stored on the SCTO’s statistics platform’s validation_tests GitHub repository\nThe repository contains both the test script(s) and the data frames used for the tests when relevant\nApproving the test is conditional on a review by a second qualified SCTO R user, from a different organizational entity (ideally, a different CTU) than the test author. Reviews are based on the “four eyes” principle.\nThe SCTO function-tests platform collects relevant meta-data for the tests including:\n\nTest author\nTest timing (date of submitting the test to the repository)\nWho reviewed the test code\nWhen was the review performed\nWhich function was tested: function name within package and package version\nWhich output of the function was tested\nWhat type of testing was performed\nTest result (pass/fail)\nEvidence of test (copy of console output)\nSession information of the testing environment\n\nThe SCTO develops a package that allows easy rerun of package tests upon change of R or package version, as well as generating a report for testing.\n\nFunction tests are stored in the validation_tests SCTO GitHub repository. Tools to create the testing structure for each tested package and run the tests are provided in the designated SCTORvalidation R add-on package. See the work instruction on writing tests for more information.\n\n5.1.1 Submitting the test for incorporation into the framework\nInstructions for writing tests and incorporating them into the validation_tests SCTO GitHub repository are provided here. Prior to incorporation, the test(s) will be reviewed according to the criteria below (“Approving the test”; Section 5.2).\n\n\n5.1.2 Running tests\nThe output from test can then be used to complete the function test issue form on GitHub. All functions from a particular R add-on package can be reported together in a single report.\nHere, we distinguish between tests that are already implemented within an R add-on package, and those that have been developed as part of the SCTO framework.\nSee the work instruction on running tests for more information.",
    "crumbs": [
      "SOPs",
      "R add-on Package Function testing"
    ]
  },
  {
    "objectID": "sop/sop_pkg_fn_testing.html#sec-approving-test",
    "href": "sop/sop_pkg_fn_testing.html#sec-approving-test",
    "title": "R add-on Package Function testing",
    "section": "5.2 Approving the test",
    "text": "5.2 Approving the test\nA consistency review is performed based on the “four eyes” principle where the reviewer should be chosen from a different participating institution than the author of the test. The review should address the following points:\n\nIs the listed meta-data complete?\nIs the level of testing specified appropriately?\nDoes the test follow the procedure described in this guideline?\n\nThis review is done in effect by approving the ‘pull request’ on GitHub and following the working instructions listed under the Reviewing tests work instruction.\nThe reviewer comments the function test. Only after approval by the reviewer, the function will be listed as tested on the SCTO platform.",
    "crumbs": [
      "SOPs",
      "R add-on Package Function testing"
    ]
  },
  {
    "objectID": "sop/sop_pkg_fn_testing.html#documentation",
    "href": "sop/sop_pkg_fn_testing.html#documentation",
    "title": "R add-on Package Function testing",
    "section": "5.3 Documentation",
    "text": "5.3 Documentation\nTest reports are made on the pkg_validation SCTO GitHub repository. Select “New issue”. Select the appropriate issue type depending on whether you’re submitting tests from within the package itself or tests from within the SCTO framework. For packages testing using tests from the validation_tests SCTO GitHub repository, fill out the predefined form, using the output from R (SCTORvalidation::test(\"packagename\")). Where packages from within the package itself are used, the work instruction provides additional information.\n\n\n\n\n\n\n\nWhat to report\nDetails\n\n\n\n\nName of tester\nWho performed the test?\n\n\nTest date\nWhen was the test performed (date)\n\n\nTested function\nWhich function from which R add-on package and package version. If the function does not come from a package (e.g. it’s a script that is stored as a GitHub Gist), the link to the code should be provided.\n\n\nTest details\nWhat precisely was tested? This could be a link to the tests or a reference to the R add-on package and function containing the tests together with that package’s version number.\n\n\nDegree Type of testing\nWhich pathway in the decision tree was followed:\n\nRe-run prior test\nReview existing R add-on package test code\nDeterministic process\nComparison to other implementation\nSimulation Was the testing comprehensive, superficial, or something in between?\n\n\n\nTest result\nPass/fail\n\n\nEvidence of test\nCopy/paste of the console output\n\n\nSessionInfo\nRelevant parts of sessionInfo():\n\nR version\nOS\nWhich other R add-on packages and versions were loaded?\n\n\n\n\nFailed tests should also be reported. When a test fails, a bug report should be posted via the appropriate route for that R add-on package (e.g. a github issue to the R add-on package repository, an email to a specific address). The bug report should also be noted alongside the test results, where possible providing a link to the report (e.g. to the GitHub issue), and followed up on by the individual discovering the bug. When the bug has been fixed, the tests can be run again and the successful test result recorded as above.",
    "crumbs": [
      "SOPs",
      "R add-on Package Function testing"
    ]
  },
  {
    "objectID": "sop/sop_pkg_fn_testing.html#footnotes",
    "href": "sop/sop_pkg_fn_testing.html#footnotes",
    "title": "R add-on Package Function testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHead data-analysis, Department Clinical Research, University of Basel↩︎\nSenior Statistician, Department of Clinical Research (DCR), University of Bern↩︎\nStatistician, Department Clinical Research, University of Basel↩︎\nHead Quality Management, Department Clinical Research, University of Bern↩︎\nIT Quality Manager, Department Clinical Research, University of Basel↩︎\nSenior Statistical Programmer, SAKK↩︎",
    "crumbs": [
      "SOPs",
      "R add-on Package Function testing"
    ]
  },
  {
    "objectID": "wi/pkg_validation.html",
    "href": "wi/pkg_validation.html",
    "title": "Risk assessing packages",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\nAuthored/revised by:\n\n\n\n\n\n\n\n\nName\nDate\n\n\n\n\nAlan Haynes,1\n2025-01-01\n\n\n\n\nVersion History\n\n\n\n\n\n\n\n\n\n\nVersion\nDate\nAuthor(s)\nSummary of Changes\n\n\n\n\n1.0\n2025-01-01\nAlan Haynes,\nInitial version\nThis page functions as the work instruction for the risk assessment of R packages within the SCTO Statistics & Methodology Platform framework.\nIn order to perform a risk assessment:",
    "crumbs": [
      "Work Intructions",
      "Risk assessing packages"
    ]
  },
  {
    "objectID": "wi/pkg_validation.html#footnotes",
    "href": "wi/pkg_validation.html#footnotes",
    "title": "Risk assessing packages",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSenior Statistician, Department of Clinical Research (DCR), University of Bern↩︎",
    "crumbs": [
      "Work Intructions",
      "Risk assessing packages"
    ]
  },
  {
    "objectID": "wi/running_tests.html",
    "href": "wi/running_tests.html",
    "title": "Running function tests",
    "section": "",
    "text": "Document development, review and version history\n\n\n\n\n\nDevelopment and Review\nAuthored/revised by:\n\n\n\n\n\n\n\n\nName\nDate\n\n\n\n\nAlan Haynes,1\n2025-01-01\n\n\n\n\nVersion History\n\n\n\n\n\n\n\n\n\n\nVersion\nDate\nAuthor(s)\nSummary of Changes\n\n\n\n\n1.0\n2025-01-01\nAlan Haynes,\nInitial version\nAlthough the unit of testing is the function, for organisational purposes, the tests are grouped by package. This is because the tests are run in the context of the package, and eases the process of running and reporting tests.\nWe distinguish between two types of tests:\nThe two methods require slightly different approaches to running the tests.",
    "crumbs": [
      "Work Intructions",
      "Running function tests"
    ]
  },
  {
    "objectID": "wi/running_tests.html#running-tests-from-within-a-package",
    "href": "wi/running_tests.html#running-tests-from-within-a-package",
    "title": "Running function tests",
    "section": "Running tests from within a package",
    "text": "Running tests from within a package\nThe easiest way to run tests from within a package is by using the same approach that the package developer uses. In the majority of cases (especially those using the testthat package), this involves downloading the package to your local machine and running the tests using the devtools package.\n\n\n\n\n\n\nDownloading from GitHub\n\n\n\n\n\nOn the package GitHub page, click on the green “Code” button and select “Download ZIP”. Unpack the ZIP file.\nIf you have git installed, you could clone the repository to your computer instead of downloading the ZIP file.\n\n\n\n\n\n\n\n\n\nDownloading from CRAN\n\n\n\n\n\nIn the downloads section of the package CRAN page, download the package source (.tar.gz file) and unpack it using 7zip. Open the file in 7zip and you should see a .tar folder, go into that and extract the contents to a folder.\n\n\n\nOnce you have the source code, in an R session, set the working directory to the package directory (e.g. open the .Rproj from the package, if it has one, or use setwd) if it does not.\nWhere packages use the testthat framework, all tests can be run via devtools::test(). Specific test files can be run using the filter argument, e.g. devtools::test(filter = \"...\"), where ... depends on the test file name (tests are typically stored in the tests/testthat folder.\nIf they do not use testthat, you will need to explore the package to find how it’s tests are run.\nTest results should be reported on the platform repository. Fill out the form with the appropriate information.",
    "crumbs": [
      "Work Intructions",
      "Running function tests"
    ]
  },
  {
    "objectID": "wi/running_tests.html#running-tests-from-within-the-scto-framework",
    "href": "wi/running_tests.html#running-tests-from-within-the-scto-framework",
    "title": "Running function tests",
    "section": "Running tests from within the SCTO framework",
    "text": "Running tests from within the SCTO framework\nTests are run via the test function in the validation package:\n\nSCTORvalidation::test(\"accrualPlot\")\n\nThe function will download the testing files from GitHub, run the tests and format the results for easier copy/pasting into a reporting issue on GitHub.\n\n\nWarning: package 'testthat' was built under R version 4.3.3\n\n\nLoading required package: accrualPlot\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'accrualPlot'\n\n\nInstalling package into 'D:/a/_temp/Library'\n(as 'lib' is unspecified)\n\n\npackage 'accrualPlot' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\runneradmin\\AppData\\Local\\Temp\\RtmpABKFBQ\\downloaded_packages\n\n\nWarning: package 'accrualPlot' was built under R version 4.3.3\n\n\nLoading required package: lubridate\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n✔ | F W  S  OK | Context\n\n⠏ |          0 | accrual_create_df                                              \n✔ |          8 | accrual_create_df\n\n⠏ |          0 | summary                                                        \n✔ |         10 | summary\n\n══ Results ═════════════════════════════════════════════════════════════════════\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 18 ]\n\n\nWarning in rm(accrualdemo): object 'accrualdemo' not found\n\n\nError in gh(endpoint = \"/user\", .token = .token, .api_url = .api_url,  : \n  GitHub API error (403): Resource not accessible by integration\nℹ Read more at\n  &lt;https://docs.github.com/rest/users/users#get-the-authenticated-user&gt;\n\n\n## Copy and paste the following output into the indicated sections of a new issue\n\nISSUE NAME: \n[Package test]: accrualPlot version 1.0.7 \n\n### Name \nrunneradmin \n\n### Name of the package you have validated \naccrualPlot \n\n### What version of the package have you validated? \n1.0.7 \n\n### Where was the package from? \nCRAN (R 4.3.3) \n\n### Package repository version reference\nNA \n\n### When was this package tested? \n2025-07-01 \n\n### What was tested? \nTests for package accrualPlot \n - `summary` produces expected results \n - `accrual_create_df` produces expected results \n  \n These tests are primarily for testing the validation infrastructure. accrualPlot \n has extensive tests \n \n\n### Test results \nPASS \n\n### Test output:\n\n\n|file                     |context           |test                     | nb| passed|skipped |error | warning|\n|:------------------------|:-----------------|:------------------------|--:|------:|:-------|:-----|-------:|\n|test-accrual_create_df.R |accrual_create_df |monocentric              |  4|      4|FALSE   |FALSE |       0|\n|test-accrual_create_df.R |accrual_create_df |multicentric             |  4|      4|FALSE   |FALSE |       0|\n|test-summary.R           |summary           |monocentric as expected  |  5|      5|FALSE   |FALSE |       0|\n|test-summary.R           |summary           |multicentric as expected |  5|      5|FALSE   |FALSE |       0|\n\n### SessionInfo:\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows Server 2022 x64 (build 20348)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: UTC\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] lubridate_1.9.4 testthat_3.2.3 \n\nloaded via a namespace (and not attached):\n [1] rappdirs_0.3.3        generics_0.1.4        tidyr_1.3.1          \n [4] stringi_1.8.7         hms_1.1.3             digest_0.6.37        \n [7] magrittr_2.0.3        RColorBrewer_1.1-3    grid_4.3.0           \n[10] evaluate_1.0.4        timechange_0.3.0      pkgload_1.4.0        \n[13] fastmap_1.2.0         rprojroot_2.0.4       jsonlite_2.0.0       \n[16] sessioninfo_1.2.3     cranlogs_2.1.1        brio_1.1.5           \n[19] conflicted_1.2.0      SCTORvalidation_0.4.3 httr_1.4.7           \n[22] purrr_1.0.4           scales_1.4.0          httr2_1.1.2          \n[25] cli_3.6.5             rlang_1.1.6           crayon_1.5.3         \n[28] gitcreds_0.1.2        withr_3.0.2           cachem_1.1.0         \n[31] yaml_2.3.10           tools_4.3.0           accrualPlot_1.0.7    \n[34] tzdb_0.5.0            memoise_2.0.1         dplyr_1.1.4          \n[37] ggplot2_3.5.2         curl_6.4.0            vctrs_0.6.5          \n[40] R6_2.6.1              lifecycle_1.0.4       stringr_1.5.1        \n[43] htmlwidgets_1.6.4     waldo_0.6.1           pkgconfig_2.0.3      \n[46] desc_1.4.3            gtable_0.3.6          pillar_1.10.2        \n[49] glue_1.8.0            gh_1.5.0              xfun_0.52            \n[52] tibble_3.3.0          tidyselect_1.2.1      knitr_1.50           \n[55] farver_2.1.2          htmltools_0.5.8.1     rmarkdown_2.29       \n[58] readr_2.1.5           pkgsearch_3.1.5       compiler_4.3.0       \n\n\n### Where is the test code located for these tests?\nSwissClinicalTrialOrganisation/validation_tests\n\n### Where the test code is located in a git repository, add the git commit SHA\n8109ba97752735e9949c347518ebc9234ca8edad\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt may be that the package being tested, or indeed functions in the testing infrastructure (testthat and waldo) have been updated since the tests were authored. This may lead to tests failing that previously passed. In this case, the tests should be updated to reflect the new behaviour.\nOne particular case is a change in attributes. For example, the coef method for a model object returns a named vector, but we might compare the individual coeffients to a vector of expected values. There is thus a potential mismatch in the attributes of the two vectors, which may cause the test to fail with a names for target but not for current message. Use of the third edition of testthat (v3.0.0) should help to avoid this issue.\n\n\nThe results of the tests should be reported on the platform repository. Copy the information returned by R to the appropriate field on the report form.",
    "crumbs": [
      "Work Intructions",
      "Running function tests"
    ]
  },
  {
    "objectID": "wi/running_tests.html#footnotes",
    "href": "wi/running_tests.html#footnotes",
    "title": "Running function tests",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSenior Statistician, Department of Clinical Research (DCR), University of Bern↩︎",
    "crumbs": [
      "Work Intructions",
      "Running function tests"
    ]
  }
]